{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######spark.ml实现Nabayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用本地spark\n",
    "#sc.stop()\n",
    "sc = SparkContext('local', 'pyspark')\n",
    "#建立spark会话\n",
    "spark = SparkSession.builder\\\n",
    "    .master('local')\\\n",
    "    .appName('spark_mllib')\\\n",
    "    .config('spark.sql.warehouse.dir', 'file:///F:/workspace/work/project/spark/spark-warehouse')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "#from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.sql.types import DoubleType\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "'''\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))\n",
    "'''\n",
    "\n",
    "# Load and parse the data\n",
    "def parseline(line):\n",
    "    values = line.split('\\t')\n",
    "    label = DoubleType(values[-1])\n",
    "    features = Vectors.dense([float(x) for x in values[:-1]])\n",
    "    return LabeledPoint(label=label, features=features)\n",
    "\n",
    "# Load and parse the data\n",
    "def parseRow(line):\n",
    "    row = line.split('\\t')\n",
    "    return Row(labelpoint=row[-1],\n",
    "               features=Vectors.dense([float(x) for x in row[:-1]]))\n",
    "\n",
    "path = r'F:\\workspace\\work\\project\\MLlib\\classification'\n",
    "data = sc.textFile(path+'\\\\'+'classify_data.txt')\n",
    "#转成classifier需要的数据格式\n",
    "parsedData = data.map(parseRow)\n",
    "dataset = parsedData.toDF()\n",
    "#from pyspark.mllib.utils import MLUtils\n",
    "#dataset.featuers = MLUtils.convertVectorColumnsFromML(dataset.featuers, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----------------+----------+\n",
      "|         features|labelpoint|\n",
      "+-----------------+----------+\n",
      "|[5.1,3.5,1.4,0.2]|         0|\n",
      "|[4.9,3.0,1.4,0.2]|         0|\n",
      "|[4.7,3.2,1.3,0.2]|         0|\n",
      "|[4.6,3.1,1.5,0.2]|         0|\n",
      "|[5.0,3.6,1.4,0.2]|         0|\n",
      "|[5.4,3.9,1.7,0.4]|         0|\n",
      "|[4.6,3.4,1.4,0.3]|         0|\n",
      "|[5.0,3.4,1.5,0.2]|         0|\n",
      "|[4.4,2.9,1.4,0.2]|         0|\n",
      "|[4.9,3.1,1.5,0.1]|         0|\n",
      "|[5.4,3.7,1.5,0.2]|         0|\n",
      "|[4.8,3.4,1.6,0.2]|         0|\n",
      "|[4.8,3.0,1.4,0.1]|         0|\n",
      "|[4.3,3.0,1.1,0.1]|         0|\n",
      "|[5.8,4.0,1.2,0.2]|         0|\n",
      "|[5.7,4.4,1.5,0.4]|         0|\n",
      "|[5.4,3.9,1.3,0.4]|         0|\n",
      "|[5.1,3.5,1.4,0.3]|         0|\n",
      "|[5.7,3.8,1.7,0.3]|         0|\n",
      "|[5.1,3.8,1.5,0.3]|         0|\n",
      "+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print type(dataset)\n",
    "print dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----+\n",
      "|         features|labelpoint|label|\n",
      "+-----------------+----------+-----+\n",
      "|[5.1,3.5,1.4,0.2]|         0|  0.0|\n",
      "|[4.9,3.0,1.4,0.2]|         0|  0.0|\n",
      "|[4.7,3.2,1.3,0.2]|         0|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|         0|  0.0|\n",
      "|[5.0,3.6,1.4,0.2]|         0|  0.0|\n",
      "|[5.4,3.9,1.7,0.4]|         0|  0.0|\n",
      "|[4.6,3.4,1.4,0.3]|         0|  0.0|\n",
      "|[5.0,3.4,1.5,0.2]|         0|  0.0|\n",
      "|[4.4,2.9,1.4,0.2]|         0|  0.0|\n",
      "|[4.9,3.1,1.5,0.1]|         0|  0.0|\n",
      "|[5.4,3.7,1.5,0.2]|         0|  0.0|\n",
      "|[4.8,3.4,1.6,0.2]|         0|  0.0|\n",
      "|[4.8,3.0,1.4,0.1]|         0|  0.0|\n",
      "|[4.3,3.0,1.1,0.1]|         0|  0.0|\n",
      "|[5.8,4.0,1.2,0.2]|         0|  0.0|\n",
      "|[5.7,4.4,1.5,0.4]|         0|  0.0|\n",
      "|[5.4,3.9,1.3,0.4]|         0|  0.0|\n",
      "|[5.1,3.5,1.4,0.3]|         0|  0.0|\n",
      "|[5.7,3.8,1.7,0.3]|         0|  0.0|\n",
      "|[5.1,3.8,1.5,0.3]|         0|  0.0|\n",
      "+-----------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#StringIndexer将一列labels转译成[0,labels基数)的index\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labeled = StringIndexer(inputCol=\"labelpoint\", outputCol=\"label\")\n",
    "datas = labeled.fit(dataset).transform(dataset)\n",
    "datas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datas_1 = datas.select(datas['features'], datas['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[5.1,3.5,1.4,0.2]|  0.0|\n",
      "|[4.9,3.0,1.4,0.2]|  0.0|\n",
      "|[4.7,3.2,1.3,0.2]|  0.0|\n",
      "|[4.6,3.1,1.5,0.2]|  0.0|\n",
      "|[5.0,3.6,1.4,0.2]|  0.0|\n",
      "|[5.4,3.9,1.7,0.4]|  0.0|\n",
      "|[4.6,3.4,1.4,0.3]|  0.0|\n",
      "|[5.0,3.4,1.5,0.2]|  0.0|\n",
      "|[4.4,2.9,1.4,0.2]|  0.0|\n",
      "|[4.9,3.1,1.5,0.1]|  0.0|\n",
      "|[5.4,3.7,1.5,0.2]|  0.0|\n",
      "|[4.8,3.4,1.6,0.2]|  0.0|\n",
      "|[4.8,3.0,1.4,0.1]|  0.0|\n",
      "|[4.3,3.0,1.1,0.1]|  0.0|\n",
      "|[5.8,4.0,1.2,0.2]|  0.0|\n",
      "|[5.7,4.4,1.5,0.4]|  0.0|\n",
      "|[5.4,3.9,1.3,0.4]|  0.0|\n",
      "|[5.1,3.5,1.4,0.3]|  0.0|\n",
      "|[5.7,3.8,1.7,0.3]|  0.0|\n",
      "|[5.1,3.8,1.5,0.3]|  0.0|\n",
      "+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print datas_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "train, test = datas.randomSplit([0.6, 0.4], seed=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----+--------------------+--------------------+----------+\n",
      "|         features|labelpoint|label|       rawPrediction|         probability|prediction|\n",
      "+-----------------+----------+-----+--------------------+--------------------+----------+\n",
      "|[4.4,3.0,1.3,0.2]|         0|  0.0|[-10.378264301029...|[0.75026434052157...|       0.0|\n",
      "|[4.4,3.2,1.3,0.2]|         0|  0.0|[-10.595284439568...|[0.77046131082683...|       0.0|\n",
      "|[4.7,3.2,1.3,0.2]|         0|  0.0|[-10.809074490258...|[0.77923452554345...|       0.0|\n",
      "|[4.7,3.2,1.6,0.2]|         0|  0.0|[-11.387299657078...|[0.74021800624473...|       0.0|\n",
      "|[4.8,3.0,1.4,0.1]|         0|  0.0|[-10.492079113781...|[0.77266770178644...|       0.0|\n",
      "|[4.8,3.0,1.4,0.3]|         0|  0.0|[-11.220039734665...|[0.72471766503686...|       0.0|\n",
      "|[4.8,3.1,1.6,0.2]|         0|  0.0|[-11.350052938039...|[0.73271058344836...|       0.0|\n",
      "|[5.0,2.0,3.5,1.0]|         1|  1.0|[-16.872904083261...|[0.12501134970361...|       1.0|\n",
      "|[5.0,2.3,3.3,1.0]|         1|  1.0|[-16.812950846523...|[0.16291131162758...|       1.0|\n",
      "|[5.0,3.0,1.6,0.2]|         0|  0.0|[-11.384069569230...|[0.72839605046135...|       0.0|\n",
      "|[5.0,3.5,1.3,0.3]|         0|  0.0|[-11.712375059200...|[0.79414950618576...|       0.0|\n",
      "|[5.1,3.3,1.7,0.5]|         0|  0.0|[-13.065545780868...|[0.67151838574419...|       0.0|\n",
      "|[5.1,3.4,1.5,0.2]|         0|  0.0|[-11.696631474265...|[0.78523263819224...|       0.0|\n",
      "|[5.1,3.8,1.9,0.4]|         0|  0.0|[-13.629599261320...|[0.72656914169998...|       0.0|\n",
      "|[5.2,3.4,1.4,0.2]|         0|  0.0|[-11.575153102222...|[0.79972216151266...|       0.0|\n",
      "|[5.2,3.5,1.5,0.2]|         0|  0.0|[-11.876404893765...|[0.79715979807919...|       0.0|\n",
      "|[5.4,3.4,1.7,0.2]|         0|  0.0|[-12.295904969502...|[0.76923177234661...|       0.0|\n",
      "|[5.4,3.9,1.3,0.4]|         0|  0.0|[-12.795449047641...|[0.81924551019490...|       0.0|\n",
      "|[5.5,2.3,4.0,1.3]|         1|  1.0|[-19.610400584913...|[0.08049302944589...|       1.0|\n",
      "|[5.6,2.5,3.9,1.1]|         1|  1.0|[-18.977981730525...|[0.12119243541391...|       1.0|\n",
      "+-----------------+----------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# train the model\n",
    "model = nb.fit(train)\n",
    "\n",
    "# select example rows to display.\n",
    "predictions = model.transform(test)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                            metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print \"Test set accuracy = \" + str(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#####参数寻优grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########模型结果展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#混淆矩阵"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
